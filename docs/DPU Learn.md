# 论文学习
## Corundum: An Open-Source 100-Gbps NIC

**摘要**：Corundum是一个基于FPGA的开源网络接口开发平台，支持高达100 Gbps及更高速率的网络接口开发。该平台包含多项核心功能以实现实时、高线速操作，包括：高性能数据路径、10G/25G/100G以太网MAC、PCI Express Gen 3、定制的PCIe DMA引擎，以及原生高精度IEEE 1588 PTP时间戳功能。其关键特性是可扩展的队列管理，支持超过10,000个队列与可扩展的传输调度器，从而实现对数据包传输的细粒度硬件控制。结合多网络接口、每接口多端口以及基于事件的端口级传输调度功能，这些特性为开发先进的网络接口、架构和协议提供了可能。软件层面通过一个针对Linux网络栈的高性能驱动与硬件交互，同时支持分散/聚集DMA、校验和卸载、接收流哈希和接收侧扩展功能。此外，平台提供了一个全面的开源Python仿真框架，覆盖从驱动和PCIe接口到以太网接口的全系统模拟，极大简化了开发和调试流程。Corundum的强大能力与灵活性通过一个微秒级精度的时分多址（TDMA）硬件调度器实现得到验证，该调度器可在100 Gbps线速下无CPU开销地执行TDMA调度。

### I. 引言与背景

网络接口控制器（NIC）是计算机与网络交互的网关。NIC在软件栈和网络之间架起桥梁，其功能定义了网络接口。网络接口的功能及其实现方式正在快速演进，这些变化由两大需求驱动：不断提升的线路速率，以及支持高性能分布式计算和虚拟化的NIC特性。更高的线路速率使得许多NIC功能必须通过硬件而非软件实现。同时，实现高级协议和网络架构需要精确的多队列传输控制等新型网络功能。

为满足在真实线路速率下开发新型网络协议和架构的开放平台需求，我们开发了一款开源的高性能FPGA基NIC原型平台Corundum。该平台支持至少94 Gbps的速率，完全开源，其驱动可跨完整网络栈使用。设计兼具可移植性和紧凑性，支持多种设备，即使在较小器件上也能保留充足资源供进一步定制。Corundum的模块化设计和可扩展性支持硬件/软件协同优化，为高级网络应用的开发和测试提供了真实环境。

### A. 动机与既有工作

通过分析现有NIC设计中硬件与软件的功能划分，可以理解开发Corundum的动机。NIC硬件功能分为两类：第一类是减轻CPU逐包处理负担的简单卸载功能（如校验和/哈希计算、分段卸载）；第二类是为实现高性能和公平性必须在硬件实现的功能（如流导向、速率限制、负载均衡和时间戳）。

传统NIC硬件功能由专用集成电路（ASIC）实现，虽能低成本高性能运行，但扩展性有限，新增功能的开发周期长且昂贵。智能NIC通过可编程处理核心和硬件原语提供灵活性，但难以支持高线路速率[1]。软件NIC通过软件实现网络功能，虽开发快捷但需消耗CPU资源且难以保证全速运行，也无法实现精确传输控制[2]。尽管存在这些限制，许多研究仍通过修改网络栈或使用DPDK等内核旁路框架在软件中实现新功能[3]-[7]。

FPGA基NIC结合了ASIC的高性能和软件NIC的灵活性：支持全线路速率、低延迟和精确时序，同时开发周期较短。阿里云等厂商开发了专有FPGA基RDMA NIC[8]，商业产品如Exablaze[9]和Netcope[10]也存在，但它们多为闭源"黑盒"，限制了新网络应用的开发灵活性。

现有高性能DMA组件（如Xilinx XDMA/QDMA核）对传输数据流的控制能力有限：XDMA核仅支持少量队列且无调度控制；QDMA和Arkville核虽面向网络应用，但分别仅支持2K和128队列，且缺乏精确传输控制方法。开源项目NetFPGA[12]提供通用FPGA报文处理工具，但其参考设计采用非网络优化的XDMA核。Catapult[13]和FlowBlaze[14]等方案将标准NIC功能交由独立ASIC实现，无法控制调度器或队列。

其他项目如Shoal、SENIC、PIEO、NDP和Loom采用部分硬件或纯软件实现，但均未在标准网络栈下实现全功能整合。Corundum的独特之处在于完全开源、支持标准网络栈下的实用线路速率，通过数千个传输队列与可扩展调度器实现细粒度流控制，成为结合硬件/软件功能的强大开发平台。

### II. 实现

Corundum具有多项独特的架构特性。首先，硬件队列状态被高效存储在FPGA的块RAM中，支持数千个独立可控队列。这些队列与接口关联，每个接口可包含多个端口，每个端口拥有独立的传输调度器，从而实现对数据包传输的极细粒度控制。调度器模块支持完全替换，可实验不同调度方案（如基于PTP时间同步的高精度TDMA）。

Corundum采用模块化参数化设计，通过Verilog参数可配置接口/端口数量、队列数、内存大小等。配置寄存器暴露参数信息，驱动无需修改即可适配不同硬件平台。当前设计支持Xilinx Ultrascale PCIe硬核，未来将扩展至更多FPGA平台。其资源占用较小，例如在Kintex Ultrascale KU035 FPGA上仅占用不到25%逻辑资源。

### A. 高层架构

如图1所示，Corundum NIC包含三级嵌套模块：

1. **顶层模块**：集成PCIe硬核、DMA接口、PTP时钟和以太网MAC/PHY。
2. **接口模块**：对应操作系统级网络接口（如eth0），管理队列状态和描述符处理。
3. **端口模块**：提供AXI Stream接口至MAC，包含调度器、收发引擎和数据路径。

### B. 流水线队列管理

通过描述符队列（主机→NIC）和完成队列（NIC→主机）协调数据传输。队列状态（128位/队列）存储在BRAM/URAM中，4096队列仅需2个URAM实例。流水线架构支持并行操作，包含四种操作：寄存器读写、出队/入队请求与提交。

### C. 传输调度器

默认采用轮询调度器（tx_scheduler_rr），其状态信息同样存储在BRAM/URAM中。调度器模块包含：

- AXI Lite接口：配置参数和队列开关。
- 流接口：接收驱动门铃事件、发送调度命令、反馈传输状态。

  支持替换为自定义调度算法（如TDMA、HPCC拥塞控制）。


### D. 端口与接口

突破传统NIC单端口单接口的限制（图2a），支持多端口绑定至同一接口（图2b）。所有端口共享传输队列，操作系统视为统一接口，通过硬件调度器实现流量的动态迁移和负载均衡，适用于P-FatTree等新型网络架构。

### E. 数据路径与收发引擎

- **接口标准**：AXI Stream用于以太网数据流，自定义分段内存接口连接PCIe DMA。
- **时钟域**：核心逻辑运行于250 MHz PCIe时钟，MAC接口异步转换（如100G CMAC运行于322 MHz）。
- **引擎功能**：
 - 发送引擎：协调校验和模块、MAC/PHY，记录PTP时间戳。
 - 接收引擎：处理流哈希，将数据写入主机内存。

### F. 分段内存接口

将PCIe接口宽度分段（如512位→8×128位），消除对齐压力，实现每周期全带宽非对齐访问。特点包括：

- 独立流控与操作排序。
- 基于选择信号（非地址解码）的路由机制。
- 字节地址映射：低4位选择段内字节，中间3位选择段，高位选择段内字地址。

### G. 设备驱动

Linux内核模块自动检测NIC参数（接口数、队列数等），管理DMA缓冲区和中断。驱动与硬件解耦，适配不同FPGA平台无需修改。

### H. 仿真框架

基于Python/MyHDL的全系统仿真，包含：

- PCIe基础设施模型（4500行代码）：根复合体、端点、中断等。
- FPGA硬核模型（4000行代码）：与Verilog设计协同仿真。

  支持快速验证不同长度数据包的收发功能。


### III. 测试结果

Corundum的100G版本在Alpha Data ADM-PCIE-9V3开发板上进行测试（搭载于双路Xeon 6138服务器），通过QSFP28铜缆连接Mellanox ConnectX-5商用NIC对比。测试使用多实例iperf3饱和链路：

1. **大包性能（MTU=9000字节）**
 - 单方向：Corundum达到95.5 Gbps（接收）/94.4 Gbps（发送），Mellanox CX5为97.8 Gbps（双向）。
 - 全双工：Corundum降至65.7/85.9 Gbps，Mellanox CX5降至83.4 Gbps，表明当前Linux内核驱动存在瓶颈。
2. **小包性能（MTU=1500字节）**
 - 单方向：Corundum为75.0/72.2 Gbps，Mellanox CX5为93.4/86.5 Gbps。
 - 提升并行传输数（8→16）后，Corundum性能从65.6/45.7 Gbps改善至75.0/72.2 Gbps，验证PCIe往返延迟是主要限制。
3. **PTP时间同步**
 - 在10G模式下，通过Arista 40G交换机作为PTP边界时钟，实现硬件时钟同步精度优于50 ns，且链路饱和时性能不变。

### IV. 案例研究：时分多址（TDMA）

Corundum通过万级队列和PTP同步实现高精度TDMA：

- **设计原理**：
 - 调度器控制模块以250 MHz运行，按PTP时间启用/禁用队列（每条队列操作耗时4 ns）。
 - 时隙长度需预留队列切换时间（如256队列禁用需1 μs）。
- **性能指标**：
 - **100G模式**：200 μs周期内划分两个100 μs时隙，控制精度达1.4 μs（2个数据包传输时间）。
 - **10G模式**：相同周期下精度为2.4 μs。





# 前置知识准备
## axis axi4 axilite（可选）协议需要大致理解，请注意，axis协议是corundum的主要传输协议，务必掌握


AXI4-Stream 是 AMBA（Advanced Microcontroller Bus Architecture）接口标准的一种，专门设计用于高性能、低延迟的通信需求，特别是在数字信号处理和视频流传输等领域。AXI4-Stream 协议主要用于在 FPGA（Field-Programmable Gate Array）和其他类型的数字硬件之间传输数据流。

AXI4-Stream 协议的主要特点包括：
流传输：AXI4-Stream 支持连续的、单向的数据流传输，适用于视频流、音频流或其他连续数据流的传输。

低延迟：通过支持突发传输（burst transfers），AXI4-Stream 能够在保持数据完整性的同时，减少传输延迟。

可配置性：AXI4-Stream 提供了多种配置选项，如数据宽度（8位、16位、32位、64位等）、传输速率等，以适应不同的应用需求。

灵活性：它支持多种传输模式，包括单向（单向传输）、单向可变（单向传输，但突发长度可变）和双向（全双工传输）。

AXI4-Stream 协议的关键参数和信号包括：
TVALID：表示传输通道中有有效数据。

TREADY：表示接收方准备好接收数据。

TDATA：携带数据的总线，其宽度根据设计需求确定。

TKEEP：指示哪些位是有效的，用于打包和解包操作。

TLAST：表示传输中的最后一个数据包。

TUSER：用户定义的信号，可用于携带额外的控制或状态信息。

TSTRB：字节使能信号，指示哪些字节是有效的。

使用示例
在 FPGA 或 SoC（System on Chip）设计中，AXI4-Stream 通常用于连接视频编解码器、图像处理器或其他需要高速数据流的应用模块。例如，一个视频流处理器可能使用 AXI4-Stream 从摄像头捕获原始视频数据，然后进行处理并输出到显示器或其他输出设备。

配置示例
假设你有一个 32 位宽的数据总线，你想配置一个 AXI4-Stream 接口：
```verilog
module axi_stream_interface (
    input wire aclk,           // 时钟信号
    input wire aresetn,        // 异步复位信号，低电平有效
    input wire [31:0] tdata,   // 数据总线
    input wire tvalid,         // 数据有效信号
    output wire tready,        // 数据接收准备好信号
    input wire [3:0] tkeep,    // 字节使能信号
    input wire tlast,          // 最后一个数据包信号
    input wire [0:0] tuser     // 用户定义信号
);
    // 在这里实现你的逻辑
endmodule
```

2. 学习计算机组成原理对开发riscv部分是有帮助的，至少要知道riscv core的组成部分，如何去修改DTS，还有openocd的概念与使用：有时间可以看看一生一芯

3. 网络驱动开发基础请大致了解，理解网卡的5个队列：发送/接收 ，发送完成、接收完成，事件队列
### 发送队列
#### 功能：存储主机准备发送出去的数据包
#### 工作流程：
  操作系统将待发送的数据包描述符放入发送队列
  网卡从队列中取出描述符并获取实际数据包
  网卡将数据包发送到网络
#### 特点：通常有多个发送队列以实现多队列并行处理

### 接收队列
#### 功能：存储从网络到达的数据包
#### 工作流程：
  网卡接收到数据包后，将其放入接收队列
  操作系统从队列中读取数据包描述符
  系统通过DMA获取实际数据包内容
#### 特点：现代网卡支持多队列接收，可实现RSS(接收侧缩放)将流量分散到不同CPU核心

### 发送完成队列
#### 功能：通知主机发送操作已完成
#### 工作流程：
  网卡成功发送数据包后，在发送完成队列中放置完成通知
  主机通过轮询或中断方式获知发送完成
  主机可以释放相关资源(如DMA缓冲区)
#### 重要性：避免过早释放资源导致的数据损坏

### 接收完成队列
#### 功能：通知主机有新数据包到达
#### 工作流程：
  网卡将接收到的数据包放入接收缓冲区后
  在接收完成队列中放置完成通知
  主机通过轮询或中断方式获知新数据包到达

### 事件队列
#### 功能：处理网卡的各种异步事件
#### 典型事件：
  链路状态变化(连接/断开)
  错误条件(如DMA错误)
  队列溢出或其他异常情况
  热插拔事件
#### 重要性：提供网卡状态监控和错误处理机制

### 队列协同工作流程
#### 发送流程：
  应用数据 → 内核协议栈 → 放入TX队列 → 网卡发送
  发送完成 → TX完成队列通知 → 释放资源
#### 接受流程
  网卡接收数据 → 放入RX队列 → RX完成队列通知
  内核读取数据 → 传递给上层应用
#### 事件处理
  网卡检测到事件 → 放入事件队列 → 触发中断或通知
  驱动处理相应事件

### 性能优化考虑
#### 多队列设计
  现代网卡支持多队列，可实现：
  RSS (Receive Side Scaling) - 基于哈希将流量分散到不同CPU
  消除锁竞争，提高并行性
#### 轮询vs中断
  高负载下轮询(DPDK风格)性能更好
  低负载下中断更节能
#### DMA使用
  队列描述符通常通过DMA与主机内存交互，减少CPU开销
4. 需要能看懂dpdk代码：请参考学长学姐的笔记
5. verilog的基本语法，基本激励文件编写

6. 对于应用层：根据自己的应用学习相关的知识，比如spark


